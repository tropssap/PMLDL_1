{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/filtered.tsv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ../data/results.tsv\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "# import torch\n",
    "#\n",
    "# device = torch.device('cuda:0')\n",
    "def load_model_and_generate_n_predictions(model_directory: str, dataset: pd.DataFrame, input_column: str, output_path: str, n: int):\n",
    "    \"\"\"\n",
    "    Load a T5 model and tokenizer, generate predictions for the first n entries of the input data, and save the predictions.\n",
    "\n",
    "    Args:\n",
    "    model_directory (str): Path to the directory where the model and tokenizer are saved.\n",
    "    dataset (pd.DataFrame): DataFrame containing the input data for prediction.\n",
    "    input_column (str): Name of the column in dataset containing the input text.\n",
    "    output_path (str): Path where the predictions should be saved.\n",
    "    n (int): Number of entries from the dataset to generate predictions for.\n",
    "    \"\"\"\n",
    "    # Load the model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_directory)\n",
    "    tokenizer = T5TokenizerFast.from_pretrained('ceshine/t5-paraphrase-paws-msrp-opinosis')\n",
    "\n",
    "    # Slice the dataset to only include the first n entries\n",
    "    dataset = dataset.head(n)\n",
    "\n",
    "    # Prepare the dataset for prediction\n",
    "    inputs = tokenizer(dataset[input_column].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate predictions\n",
    "    output_sequences = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "\n",
    "    # Decode the predictions into text\n",
    "    predictions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n",
    "\n",
    "    # Save the predictions to a file\n",
    "    predictions_df = pd.DataFrame({input_column: dataset[input_column], 'prediction': predictions})\n",
    "    predictions_df.to_csv(output_path, index=False, sep='\\t')\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "load_model_and_generate_n_predictions('../models/t5-cechine-nmt-mined-detox1', df, 'reference', '../data/results.tsv',n=100)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\igoli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come on, you little punk, you punk, you punk, you punk\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def generate_prediction_from_text(model_directory: str, input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a T5 model and tokenizer, generate a prediction for the input text.\n",
    "\n",
    "    Args:\n",
    "    model_directory (str): Path to the directory where the model and tokenizer are saved.\n",
    "    input_text (str): Text to generate prediction from.\n",
    "\n",
    "    Returns:\n",
    "    str: The model's predicted output.\n",
    "    \"\"\"\n",
    "    # Load the model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_directory)\n",
    "    tokenizer = T5Tokenizer.from_pretrained('ceshine/t5-paraphrase-paws-msrp-opinosis')\n",
    "\n",
    "    # Tokenize the input text for the model\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate predictions\n",
    "    output_sequences = model.generate(input_ids=input_ids)\n",
    "\n",
    "    # Decode the prediction into text\n",
    "    prediction = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "prediction = generate_prediction_from_text('../models/t5-cechine-nmt-mined-detox1', \"You motherfucker, come on you little ass… fuck with me, eh? You fucking little asshole, dickhead cocksucker…You fuckin' come on, come fuck with me! I'll get your ass, you jerk! Oh, you fuckhead motherfucker! Fuck all you and your family! Come on, you cocksucker, slime bucket, shitface turdball! Come on, you scum sucker, you fucking with me? Come on, you asshole\")\n",
    "print(prediction)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
